# Recent Advances in Vision and Language PreTrained Models (VL-PTMs)
Maintained by [WANG Yue](https://yuewang-cuhk.github.io/) (yuewang@cse.cuhk.edu.hk). Last update on 2020/03/25.

## Table of Contents

* [Image-based VL-PTMs](#image-based-vl-ptms)
  * [General](#general)
  * [Task-specifc](#task-specifc)
  * [Multi-task Learning](#multi-task-learning)
* [Video-based VL-PTMs](#video-based-vl-ptms)


# Image-based VL-PTMs

## General

[ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](https://arxiv.org/abs/1908.02265), NeurIPS 2019 [[code]](https://github.com/jiasenlu/vilbert_beta)

[LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490), EMNLP 2019 [[code]](https://github.com/airsplay/lxmert)

[VL-BERT: Pre-training of Generic Visual-Linguistic Representations](https://arxiv.org/abs/1908.08530), ICLR 2020
 [[code]](https://github.com/jackroos/VL-BERT)

[VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/abs/1908.03557), arXiv 2019/08 [[code]](https://github.com/uclanlp/visualbert)

[Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training](https://arxiv.org/abs/1908.06066), AAAI 2020

[Unified Vision-Language Pre-Training for Image Captioning and VQA](https://arxiv.org/pdf/1909.11059.pdf), AAAI 2020, [[code]](https://github.com/LuoweiZhou/VLP)

[UNITER: Learning Universal Image-text Representations](https://arxiv.org/abs/1909.11740), arXiv 2019/09 [[code]](https://github.com/ChenRocks/UNITER)

## Task-specifc

[Fusion of Detected Objects in Text for Visual Question Answering](https://arxiv.org/abs/1908.05054), EMNLP 2019, [[code]](https://github.com/google-research/language/tree/master/language/question_answering/b2t2)

[Iterative Answer Prediction with Pointer-Augmented
Multimodal Transformers for TextVQA](https://arxiv.org/abs/1911.06258), CVPR 2020

[Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline](https://arxiv.org/abs/1912.02379), arXiv 2019/12, [[code]](https://github.com/vmurahari3/visdial-bert)

[Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training] (https://arxiv.org/abs/2002.10638), CVPR 2020, [[code]](https://github.com/weituo12321/PREVALENT)

[ImageBERT: Cross-Modal Pre-training with
Large-scale Weak-supervised Image-text Data](https://arxiv.org/abs/2001.07966), arXiv 2020/01

[XGPT: Cross-modal Generative Pre-Training for Image Captioning](https://arxiv.org/abs/2003.01473), arXiv 2020/03

## Multi-task Learning

[12-in-1: Multi-Task Vision and Language Representation Learning](https://arxiv.org/abs/1912.02315), CVPR 2020, [[code]](https://github.com/facebookresearch/vilbert-multi-task) 

[All-in-One Image-Grounded Conversational Agents](https://arxiv.org/abs/1912.12394), arXiv 2019/12


# Video-based VL-PTMs

[VideoBERT: A Joint Model for Video and Language Representation Learning](https://arxiv.org/abs/1904.01766), ICCV 2019

[Learning Video Representations Using
Contrastive Bidirectional Transformers](https://arxiv.org/abs/1906.05743), arXiv 2019/06

[UniViLM: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation](https://arxiv.org/abs/2002.06353), arXiv 2020/02